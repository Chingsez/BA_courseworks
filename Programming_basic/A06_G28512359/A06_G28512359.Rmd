---
title: "A06_G28512359"
author: "JINGSI WU"
date: "10/6/2018"
output: html_document
---
#Import data
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(car)
train <- read.csv('train.csv',stringsAsFactors = FALSE, na.strings = "")
```

# Part A Data cleaning
## Omit some coloumns and NA data

```{r information}
head(train)
str(train)
```


By looking at some information about our dataset, we can see that there are some NA values, also there must be some redundant variables, but since most of the nominal variables have been turn to dummy variables, we can only remove ID and Product_Info_2 first, then remove the columns with NA.

```{r missing value}
train <- subset(train, select = -c(Id, Product_Info_2 ))
train_flag <- apply(is.na(train), 2, sum)
train <- train[, which(train_flag == 0)]
```


##Fit multiple logistic regression model
Now let's fit the remaining variables in to a multiple linear regression model.

```{r regression}
train_model <- glm(formula = Response ~ .,  data = train)
summary(train_model)
```

We can see that not all of the variables are significantly related to the model, we'll remove these variables
```{r subset}
train <- subset(train, select = -c(Product_Info_1, Product_Info_5, Product_Info_7, Employment_Info_2, Employment_Info_5, InsuredInfo_3, InsuredInfo_4, Insurance_History_3, Insurance_History_4, Insurance_History_7, Insurance_History_9, Family_Hist_1, Medical_History_6, Medical_History_8, Medical_History_9, Medical_History_16, Medical_History_20, Medical_History_21, Medical_History_25, Medical_History_26, Medical_History_33, Medical_History_34, Medical_History_36, Medical_Keyword_1, Medical_Keyword_4, Medical_Keyword_5, Medical_Keyword_7, Medical_Keyword_8, Medical_Keyword_10, Medical_Keyword_11, Medical_Keyword_13, Medical_Keyword_14, Medical_Keyword_16, Medical_Keyword_17, Medical_Keyword_18, Medical_Keyword_20, Medical_Keyword_21, Medical_Keyword_23, Medical_Keyword_24, Medical_Keyword_27, Medical_Keyword_28, Medical_Keyword_29, Medical_Keyword_30, Medical_Keyword_32, Medical_Keyword_35, Medical_Keyword_36, Medical_Keyword_40, Medical_Keyword_42, Medical_Keyword_43, Medical_Keyword_44, Medical_Keyword_46, Medical_Keyword_47, Medical_Keyword_48, Medical_History_37, Medical_Keyword_12, Medical_Keyword_26))
```

##Use stepwise and VIF to remove variables
Then we'll use stepwise method and checking VIF to remove other redundant variables. 
```{r logistic regression}
train_model <- glm(formula = Response ~ .,  data = train)
summary(train_model)
```


```{r stepwise}
library(MASS)
stepwise <- stepAIC(train_model, direction = "both")
```


```{r remove variables}
train <- subset(train, select = c(Response, Product_Info_3, Product_Info_4, Product_Info_6, 
    Ins_Age, Ht, Wt, BMI, Employment_Info_3, InsuredInfo_1, 
    InsuredInfo_2, InsuredInfo_5, InsuredInfo_6, InsuredInfo_7, 
    Insurance_History_1, Insurance_History_2, Insurance_History_8, 
    Medical_History_2, Medical_History_3, Medical_History_4, 
    Medical_History_5, Medical_History_7, Medical_History_11, 
    Medical_History_12, Medical_History_13, Medical_History_14, 
    Medical_History_17, Medical_History_18, Medical_History_19, 
    Medical_History_22, Medical_History_23, Medical_History_27, 
    Medical_History_28, Medical_History_29, Medical_History_30, 
    Medical_History_31, Medical_History_35, Medical_History_38, 
    Medical_History_39, Medical_History_40, Medical_History_41, 
    Medical_Keyword_2, Medical_Keyword_3, Medical_Keyword_6, 
    Medical_Keyword_9, Medical_Keyword_15, Medical_Keyword_19, 
    Medical_Keyword_22, Medical_Keyword_25, Medical_Keyword_31, 
    Medical_Keyword_33, Medical_Keyword_34, Medical_Keyword_37, 
    Medical_Keyword_38, Medical_Keyword_39, Medical_Keyword_41, 
    Medical_Keyword_45))
```

```{r VIF}
train_model <- glm(formula = Response~.,  data = train)
vif(train_model)
```

By omiting all the variables above we can get the final dataset for our model.
```{r final dataset}
train <- subset(train, select = -c(Ht, Wt, BMI))
```

# Part B Visualizations
For the remaining variables, we want to know more about the relationship between dependent variables and continuous independent variables, so we choose Product_Info_4 and Ins_Age for visualization.
```{r visualization}
train_reg <- lm(Response~Product_Info_4+Ins_Age, data = train)
summary(train_reg)
plot(train_reg)
```

From the picture and the summary we can conclude that both two variables can significantly explain the value of Response, but not as a linear regression model, so now we are going to fit the whole dataset with other machine learning model.

#Part C Fit dataset with decision tree model
```{r decision tree}
library(rpart)
train_tree <- rpart(formula= Response~., data=train, method = "class")
test <- read.csv('test.csv',stringsAsFactors = FALSE, na.strings = "")
prediction <- predict(train_tree, test,type = 'class')
predictresult<-as.matrix(prediction)
colnames(predictresult)<-'Response'
output<-cbind(test$Id, predictresult)
write.csv(output,'prediction.csv',row.names = FALSE)
```

