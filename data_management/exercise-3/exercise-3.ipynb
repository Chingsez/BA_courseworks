{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "*Objectives*: Wrangle a data set using two new tools, [Trifacta Wrangler](https://www.trifacta.com/start-wrangling/) and [Apache Spark](https://spark.apache.org/).  Results should include a cleaned-up data set and summary statistics.\n",
    "\n",
    "*Grading criteria*: The tasks should all be completed, and questions should all be answered with clear responses, with shell commands and markdown cells explaining your work as appropriate in the cells provided (as more as needed).  The notebook itself should be completely reproducible (using AWS an EC2 instance based on the class AMI) from start to finish; another person should be able to use the code to obtain the same results as yours.  Note that you will receive no more than partial credit if you do not fill in the text/markdown cells provided explaining your thinking where required.\n",
    "\n",
    "*Attestation*: **Work individually**.  At the end of your submitted notebook, state that you did all of the substantial work on this assignment yourself, and acknowledge any assistance you received.\n",
    "\n",
    "*Deadline*: Monday, November 5, 1pm.  Zip your notebook and wrangled dataset and submit it to Blackboard as a single zip (`.zip`) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Wrangle a dataset with Trifacta\n",
    "\n",
    "For this part, select a CSV dataset from the [OKFN US City Open Data Census](http://us-cities.survey.okfn.org/).  Choose one according to your interest, but try to choose one that's \"green\" and has somewhere between 10,000 and 1,000,000 rows.  Try also to choose a dataset that is less than 50MB (to save us some time and space during grading!).\n",
    "\n",
    "Document your process by answering each of the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1 - Choose your dataset\n",
    "\n",
    "Which dataset did you choose?  What is it called, and what is it about?  Provide a link to its main web page (not its data link, which you'll include next).\n",
    "\n",
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset I choose is the 2017 employee salaries in Baltimore,MD. It collect a bunch of data about some eyployee's salaries in Baltimore, inculding there name, there working information, when they was hired, their annual salary, and the gross salaries.\n",
    "\n",
    "link : http://us-cities.survey.okfn.org/entry/baltimore/employee-salaries/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2 - Get your data\n",
    "\n",
    "If possibly, use `wget` to download your data onto your instance. **If you cannot**, make sure that link you provided works, note that it will need to be uploaded manually, and upload it manually so you can inspect it in the next sections.\n",
    "\n",
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-11-02 13:53:04--  https://data.baltimorecity.gov/api/views/fh59-3d3c/rows.csv\n",
      "Resolving data.baltimorecity.gov (data.baltimorecity.gov)... 52.206.140.205, 52.206.140.199, 52.206.68.26\n",
      "Connecting to data.baltimorecity.gov (data.baltimorecity.gov)|52.206.140.205|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/csv]\n",
      "Saving to: 'rows.csv'\n",
      "\n",
      "rows.csv                [ <=>                ]   1.46M  --.-KB/s    in 0.03s   \n",
      "\n",
      "Last-modified header invalid -- time-stamp ignored.\n",
      "2018-11-02 13:53:04 (46.6 MB/s) - 'rows.csv' saved [1526371]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget  https://data.baltimorecity.gov/api/views/fh59-3d3c/rows.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Use command line tools of your choice (CSVKit, XSV, or other UNIX commands we've seen in class already) to explore your data.  How long is it?  Does it seem relatively clean? Do you see data issues that need wrangling?\n",
    "\n",
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. \"NAME\"\n",
      "\n",
      "\tType of data:          Text\n",
      "\tContains null values:  False\n",
      "\tUnique values:         13439\n",
      "\tLongest value:         31 characters\n",
      "\tMost common values:    Baylor Thompson,Joyce M (2x)\n",
      "\t                       Brown,Kevin M (2x)\n",
      "\t                       Brown,Robert L (2x)\n",
      "\t                       Canan,Ruth E (2x)\n",
      "\t                       Carter,Angela (2x)\n",
      "\n",
      "  2. \"JOBTITLE\"\n",
      "\n",
      "\tType of data:          Text\n",
      "\tContains null values:  False\n",
      "\tUnique values:         1077\n",
      "\tLongest value:         30 characters\n",
      "\tMost common values:    Police Officer (1514x)\n",
      "\t                       Laborer (Hourly) (556x)\n",
      "\t                       RECREATION ARTS INSTRUCTOR (366x)\n",
      "\t                       EMT Firefighter Suppression (300x)\n",
      "\t                       OFFICE SUPPORT SPECIALIST III (280x)\n",
      "\n",
      "  3. \"DEPTID\"\n",
      "\n",
      "\tType of data:          Text\n",
      "\tContains null values:  False\n",
      "\tUnique values:         665\n",
      "\tLongest value:         6 characters\n",
      "\tMost common values:    P04001 (368x)\n",
      "\t                       C90786 (241x)\n",
      "\t                       P04002 (230x)\n",
      "\t                       A99416 (157x)\n",
      "\t                       A64003 (134x)\n",
      "\n",
      "  4. \"DESCR\"\n",
      "\n",
      "\tType of data:          Text\n",
      "\tContains null values:  False\n",
      "\tUnique values:         664\n",
      "\tLongest value:         30 characters\n",
      "\tMost common values:    R&P-Recreation (part-time) ( (598x)\n",
      "\t                       TRANS-Crossing Guards (786) (241x)\n",
      "\t                       Police Department (416) (157x)\n",
      "\t                       Fire Department (003) (134x)\n",
      "\t                       General Services (301) (132x)\n",
      "\n",
      "  5. \"HIRE_DT\"\n",
      "\n",
      "\tType of data:          DateTime\n",
      "\tContains null values:  False\n",
      "\tUnique values:         4639\n",
      "\tSmallest value:        1962-04-03 00:00:00\n",
      "\tLargest value:         2017-10-02 00:00:00\n",
      "\tMost common values:    2017-05-27 00:00:00 (90x)\n",
      "\t                       2007-06-23 00:00:00 (66x)\n",
      "\t                       2006-07-01 00:00:00 (52x)\n",
      "\t                       2007-12-12 00:00:00 (49x)\n",
      "\t                       2017-07-12 00:00:00 (49x)\n",
      "\n",
      "  6. \"ANNUAL_RT\"\n",
      "\n",
      "\tType of data:          Number\n",
      "\tContains null values:  False\n",
      "\tUnique values:         1769\n",
      "\tSmallest value:        1,800\n",
      "\tLargest value:         250,000\n",
      "\tSum:                   740,215,086\n",
      "\tMean:                  54,899.88\n",
      "\tMedian:                50,656\n",
      "\tStDev:                 25,137.618\n",
      "\tMost common values:    20,800 (269x)\n",
      "\t                       19,240 (197x)\n",
      "\t                       65,009 (196x)\n",
      "\t                       48,971 (149x)\n",
      "\t                       30,430 (139x)\n",
      "\n",
      "  7. \"Gross\"\n",
      "\n",
      "\tType of data:          Number\n",
      "\tContains null values:  True (excluded from calculations)\n",
      "\tUnique values:         12611\n",
      "\tSmallest value:        0\n",
      "\tLargest value:         244,913.7\n",
      "\tSum:                   747,651,058.04\n",
      "\tMean:                  58,405.676\n",
      "\tMedian:                54,519.92\n",
      "\tStDev:                 33,524.097\n",
      "\tMost common values:    None (682x)\n",
      "\t                       9,948.05 (19x)\n",
      "\t                       0 (8x)\n",
      "\t                       10,527.69 (7x)\n",
      "\t                       10,221.32 (7x)\n",
      "\n",
      "Row count: 13483\n"
     ]
    }
   ],
   "source": [
    "!csvstat rows.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!csvcut -n rows.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result we can see that this dataset contains 7 column. There are some null vaule in the Gross column, also the format of HIRE_DT is not correct, time format seems to be redundant here. Wr can use Trifacta to change the format and fill the null value with ANNUAL_RT(will have some error, but relatively close). The Name, Jobtitle and Department will be annoyed when convert to RDD, so they are all removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.4 - Wrangle your data with Trifacta\n",
    "\n",
    "Use Trifacta to import your data.  You will have to create an account, which is free, to use Trifacta Wrangler.  \n",
    "\n",
    "Find **at least two columns** you want to wrangle and clean them up - you can split values into new columns, remove bad values, whatever you like.\n",
    "\n",
    "Execute your recipe, generating a summary you can review, and download your recipe.\n",
    "\n",
    "Paste the text of your recipe into the cell below using the markdown provided.\n",
    "\n",
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "dateformat col: HIRE_DT type: slashdate\n",
    "set col: Gross value: IFMISSING($col, ANNUAL_RT)\n",
    "drop col: NAME action: Drop\n",
    "drop col: JOBTITLE action: Drop\n",
    "drop col: DESCR action: Drop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.5 - Evaluate\n",
    "\n",
    "How did it go?  Did your recipe work on the whole dataset?  Did you run into any problems?\n",
    "\n",
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recipe worked smoothly, although it took a bit long since the dataset is relatively large, but finally I got the dataset I want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Summary statistics with Spark\n",
    "\n",
    "Use Spark to load your data and compute basic summary statistics (counts, or average, min/max, and mean).  You may borrow liberally from the example we saw in class, just change a few things as appropriate.\n",
    "\n",
    "This is just to get you a taste... we'll do more with Spark next week and in Project 3.\n",
    "\n",
    "### Q2.1 - Start Spark\n",
    "\n",
    "First, load up Spark by executing the following cells.  You can just execute them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkContext(appName='exercise-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-88-167:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>exercise-3</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=exercise-3>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it worked, you should see the description of your **SparkContext** and a link (that you can visit by replacing its IP address with your EC2 instance host name).\n",
    "\n",
    "### Q2.2 - Upload your wrangled data\n",
    "\n",
    "Upload the data you wrangled with Trifacta in Part 1.  You may use Jupyter's upload function for this, it doesn't need to be captured here.  You may want to compress your data before uploading it.\n",
    "\n",
    "In a few cells below, ensure that your data uploaded correctly, and uncompress it if necessary.  Count its lines, check its filesize, or look at the first few lines as you deem appropriate until you're confident you have all your data to use here in the notebook.\n",
    "\n",
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. \"DEPTID\"\n",
      "\n",
      "\tType of data:          Text\n",
      "\tContains null values:  False\n",
      "\tUnique values:         665\n",
      "\tLongest value:         6 characters\n",
      "\tMost common values:    P04001 (368x)\n",
      "\t                       C90786 (241x)\n",
      "\t                       P04002 (230x)\n",
      "\t                       A99416 (157x)\n",
      "\t                       A64003 (134x)\n",
      "\n",
      "  2. \"HIRE_DT\"\n",
      "\n",
      "\tType of data:          Date\n",
      "\tContains null values:  False\n",
      "\tUnique values:         4639\n",
      "\tSmallest value:        1962-04-03\n",
      "\tLargest value:         2017-10-02\n",
      "\tMost common values:    2017-05-27 (90x)\n",
      "\t                       2007-06-23 (66x)\n",
      "\t                       2006-07-01 (52x)\n",
      "\t                       2007-12-12 (49x)\n",
      "\t                       2017-07-12 (49x)\n",
      "\n",
      "  3. \"ANNUAL_RT\"\n",
      "\n",
      "\tType of data:          Number\n",
      "\tContains null values:  False\n",
      "\tUnique values:         1769\n",
      "\tSmallest value:        1,800\n",
      "\tLargest value:         250,000\n",
      "\tSum:                   740,215,086\n",
      "\tMean:                  54,899.88\n",
      "\tMedian:                50,656\n",
      "\tStDev:                 25,137.618\n",
      "\tMost common values:    20,800 (269x)\n",
      "\t                       19,240 (197x)\n",
      "\t                       65,009 (196x)\n",
      "\t                       48,971 (149x)\n",
      "\t                       30,430 (139x)\n",
      "\n",
      "  4. \"Gross\"\n",
      "\n",
      "\tType of data:          Number\n",
      "\tContains null values:  False\n",
      "\tUnique values:         12768\n",
      "\tSmallest value:        0\n",
      "\tLargest value:         250,000\n",
      "\tSum:                   773,800,511.04\n",
      "\tMean:                  57,390.826\n",
      "\tMedian:                52,921.34\n",
      "\tStDev:                 33,499.858\n",
      "\tMost common values:    20,800 (75x)\n",
      "\t                       19,240 (57x)\n",
      "\t                       24,960 (51x)\n",
      "\t                       48,971 (46x)\n",
      "\t                       28,488 (24x)\n",
      "\n",
      "Row count: 13483\n"
     ]
    }
   ],
   "source": [
    "!csvstat Baltimore.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: DEPTID\n",
      "  2: HIRE_DT\n",
      "  3: ANNUAL_RT\n",
      "  4: Gross\n"
     ]
    }
   ],
   "source": [
    "!csvcut -n Baltimore.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3 - Load your data into a Spark RDD\n",
    "\n",
    "Load up your data using the techniques we reviewed in class.  Extract the header. Get a count to verify that it's working correctly.\n",
    "\n",
    "Modify the cells below to get started.\n",
    "\n",
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Edit this cell to point to your file!\n",
    "data = spark.textFile('Baltimore.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"DEPTID\",\"HIRE_DT\",\"ANNUAL_RT\",\"Gross\"'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = data.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13484"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.4 - Summarize your data\n",
    "\n",
    "Choose one of the two techniques we saw in class to compute some basic numbers on one of your columns.  Your options are:\n",
    "\n",
    " * Use `map` and `filter` and `reduceByKey` with `lambda` functions find min/max values and to count frequencies in one column\n",
    " * Use the `Statistics` module to compute count, mean, min/max (don't forget to import it and numpy)\n",
    " \n",
    "It's your choice.\n",
    "\n",
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the data we first use spark to see which 10 salary amount is most frequent in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salary_top10 = data.filter(lambda row: row != header) \\\n",
    "    .map(lambda row: row.split(\",\")) \\\n",
    "    .map(lambda cols: (cols[2], 1)) \\\n",
    "    .reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\"20800.00\"', 269),\n",
       " ('\"19240.00\"', 197),\n",
       " ('\"65009.00\"', 196),\n",
       " ('\"48971.00\"', 149),\n",
       " ('\"30430.00\"', 139),\n",
       " ('\"24960.00\"', 134),\n",
       " ('\"31345.00\"', 118),\n",
       " ('\"83881.00\"', 110),\n",
       " ('\"32260.00\"', 100),\n",
       " ('\"10864.00\"', 93)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_top10.takeOrdered(10, key=lambda r: -r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary = data.filter(lambda row: row != header) \\\n",
    "    .map(lambda row: row.split(\",\"))\\\n",
    "    .map(lambda col: float(col[2].replace('\"', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[57863.0,\n",
       " 78600.0,\n",
       " 54486.0,\n",
       " 37415.0,\n",
       " 72800.0,\n",
       " 65009.0,\n",
       " 67218.0,\n",
       " 83576.0,\n",
       " 45893.0,\n",
       " 22464.0]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250000.0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800.0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.5 - Evaluate\n",
    "\n",
    "How did it go?  Did it work as you expected?  Did you run into any issues?\n",
    "\n",
    "What do you like about using Spark?  Or do you dislike it?\n",
    "\n",
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When triing to get the max/min/mean value of the data, I found spark went wrong sometimes, although I do choose the right column and try to convert it into float in order to compare, after looking back to the csv file, I found 'NAME', 'JOBTITLE' and 'DESCR' column is annoying when covert csv file to RDD, so they must be removed, and this do solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
